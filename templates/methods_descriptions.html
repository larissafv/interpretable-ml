<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Methods — Interpretable ML</title>
    <meta name="description" content="Descriptions of the explanation methods available in this demo." />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="/static/css/home.css" />
    <link rel="stylesheet" href="/static/css/form.css" />
    <style>
      .lead { margin-top: 6px; color: var(--muted); }
      .methods { display: grid; gap: 18px; grid-template-columns: 1fr; }
      .method-summary { display:flex; gap:14px; align-items:flex-start }
      .method-summary h3 { margin:0 0 6px 0 }
      .kbd { background:#f3f4f6; border-radius:4px; padding:2px 6px; font-family:monospace }
    </style>
  </head>
  <body>
    <header class="site-header">
      <div class="container header-inner">
        <div class="brand">
          <a class="brand-link" href="/" aria-label="Back to home">
            <span class="brand-mark">∫</span>
            <span class="brand-name">Interpretable ML</span>
          </a>
        </div>
        <nav class="top-nav" aria-label="Primary">
          <a class="nav-link" href="/methods-descriptions">Methods details</a>
          <a class="nav-link" href="/instructions">User guide</a>
          <a class="nav-link" href="#about">About us</a>
        </nav>
      </div>
    </header>

    <main id="content" class="site-main">
      <section class="container">
        <h1 class="page-title">Explanation methods (overview)</h1>
        <p class="lead">Short, user-friendly descriptions of each method available in the demo. Click a method on the home page to configure and run it.</p>

        <div class="methods">
          <article class="panel">
            <div class="method-summary">
              <div style="flex:1">
                <h3>LIME (Local Interpretable Model-agnostic Explanations)</h3>
                <p>LIME explains individual predictions by perturbing the input around the instance of interest and fitting a simple interpretable model (linear regression) to the perturbed samples. The learned linear model's coefficients indicate which features (timepoints or variates) influenced the prediction locally.</p>
                <p>Key parameters:</p>
                <ul>
                  <li><strong>Type</strong> (<code>ecg</code> / <code>with_slices</code> / <code>general</code>): controls how perturbations are generated.</li>
                  <li><strong>n_samples</strong>: number of perturbed samples to generate (larger → more stable coefficients, but slower).</li>
                  <li><strong>top_features_percentage</strong>: fraction of top points highlighted in the visual summary.</li>
                </ul>
                <p>Outputs: heatmap and highlighted-timeseries plots that show which parts of the signal most influenced the prediction.</p>
              </div>
            </div>
          </article>

          <article class="panel">
            <div class="method-summary">
              <div style="flex:1">
                <h3>Counterfactual Explanations (Genetic search)</h3>
                <p>This method searches for small changes to an input instance that change the model's predicted class. The implementation uses a genetic algorithm that mutates, crosses and selects candidate solutions to find counterfactuals that flip the prediction while keeping edits sparse and minimal.</p>
                <p>Key parameters:</p>
                <ul>
                  <li><strong>ce_type</strong> (<code>ecg</code> / <code>with_slices</code> / <code>general</code>): controls mutation/perturbation strategy.</li>
                  <li><strong>pop_size</strong>, <strong>n_generations</strong>: control the GA search cost and thoroughness.</li>
                  <li>Genetic operator probabilities (<code>prob_crossover</code>, <code>prob_mutation</code>) and selection (<code>tournament_size</code>).</li>
                </ul>
                <p>Outputs: a plot comparing the original instance to the generated counterfactual(s) (differences highlighted).</p>
              </div>
            </div>
          </article>

          <article class="panel">
            <div class="method-summary">
              <div style="flex:1">
                <h3>Global Surrogate</h3>
                <p>Global surrogate trains an interpretable surrogate model (for example, a small decision tree) on inputs and predictions produced by the black-box model. The surrogate approximates the global decision boundaries and can be inspected to understand overall model behavior.</p>
                <p>Key parameters: surrogate model type, training settings, and whether to use original labels or model predictions.</p>
                <p>Outputs: surrogate performance metrics and visualization of surrogate decision rules (if available).</p>
              </div>
            </div>
          </article>

          <article class="panel">
            <div class="method-summary">
              <div style="flex:1">
                <h3>Per-point importance</h3>
                <p>Per-point importance computes local importance scores by perturbing individual timepoints (or small groups/slices) of a single instance and measuring the change in the model's prediction. This is useful to obtain high-resolution, pointwise explanations for time-series inputs.</p>
                <p>Key parameters: perturbation scheme (single-point vs. slice), number of repeats, and scoring metric used to quantify prediction change.</p>
                <p>Outputs: per-point importance arrays and visual overlays that highlight which timepoints the model relied on for the prediction.</p>
              </div>
            </div>
          </article>

          <article class="panel">
            <div class="method-summary">
              <div style="flex:1">
                <h3>Counterfactual Explanation with Per-point importance</h3>
                <p>This hybrid flow combines counterfactual search with per-point importance to produce counterfactuals that are better explained at the point level. Use per-point importance to prioritise or constrain candidate edits and obtain more interpretable, sparse counterfactuals.</p>
                <p>Key parameters: those of the counterfactual search (population, generations, operator probabilities) plus the PPI perturbation settings that steer or evaluate candidate changes.</p>
                <p>Outputs: counterfactual instances accompanied by per-point importance overlays that show which timepoints were changed and how the model's reliance shifted.</p>
              </div>
            </div>
          </article>
        </div>
      </section>
    </main>

    <footer class="site-footer">
      <div class="container footer-inner">
        <p>© <span id="year"></span> Interpretable ML</p>
      </div>
    </footer>
    <script>document.getElementById('year').textContent = new Date().getFullYear();</script>
  </body>
</html>
